{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"PHYS3080: Distance Ladder Project","text":""},{"location":"#context","title":"Context","text":"<p>The planet of New Earth is engulfed in one of the thickest atmospheres in the Universe. Although New Earthlings are highly intelligent, their concept of astronomy is relatively new. When their first rockets penetrated the clouds and snapped pictures of the sky above, they discovered their planet was orbiting a star (along with other planets) and the sky was filled with other sources of light (other stars?).</p> <p>Many rockets later, the New Earthlings have gained a good understanding of their own solar system. Very little, however, is known about the rest of their Universe. It is known that there are many dots of light, and some small fuzzy glowing patches, which do not seem to move across the sky, and which are probably not part of the solar system. Spectra have been obtained of a few of these dots and fuzzy patches - the spectra look like black body curves with absorption-lines superimposed. They seem to have a range of temperatures (typically a few thousand K), and to be made of elements such as Hydrogen and Carbon, similar to New Earth's parent Sun.</p> <p>Over the last few years, you have helped build the planet's most ambitious (and expensive) space-probe yet: the Pimbblet Picture Producer (named after an Antipodean scientist with a rare surname who liked thinking about BIG questions). This remarkable satellite contained a number of instruments:</p> <ul> <li>Six wide-field cameras, each of which repeatedly took pictures of 1/6 of the sky. Together, they measured the brightness and position of each object in the sky repeatedly. Variable objects were automatically identified and light curves generated for them.</li> <li>A high resolution, narrow-field camera, which took more detailed close-up images of smaller regions. It took two images of each part of the sky, spaced by six months. It detects every object brighter than 10<sup>-22</sup> W m<sup>-2</sup> nm<sup>-1</sup> in its field of view. Its six month spacing also allowed it to measure the parallax of each source, as New Earth moves around its Sun (New Earth is 1 AU from its Sun, and orbits it once a year).</li> <li>A spectrometer, which measured the wavelengths of the various absorption-lines, and hence used the Doppler effect to determine the radial velocity of each object detected by the imaging cameras.</li> <li>An X-ray all-sky monitor which had 1m<sup>2</sup> detector area, which scanned the sky looking for any X-rays coming from space.</li> </ul> <p>All these instruments were programmed to not take data whenever anything in New Earth's solar system was in their field of view. This was to prevent the cameras being damaged by such bright targets. As a result, you will not see anything from New Earth's solar system in these data.</p> <p>The space-probe has just landed after its one year mission, and the data tapes eagerly retrieved by your team. You have just started to analyse these exciting and unprecedented data.</p>"},{"location":"#sitemap","title":"Sitemap","text":"<p>On the Introduction page you'll find all of the physics that is actually emulated in this simulation, as well as some important information about the simulation on the Data Structure page. Check your group identification and dataset on the datasets page, and the data structure on the data page.</p>"},{"location":"#hints","title":"Hints","text":"<p>Your job, in general terms, is to work out as much as you can about New Earth's Universe. You can assume that basic physical constants like the speed of light or the constant of gravity are the same as in our universe. However, the more complicated physics like star formation, stellar and galaxy evolution are probably different. To what extent they differ is something that you will have to work out. The more you work out, the better your score in this assignment will be. Here are some some unsorted, non-exhaustive hints and pointers about the exercise:</p> <p>There are no marks for just coming up with lists of numbers -- we are interested in the physics of your given Universe.</p> <ul> <li>Does your Universe contain galaxies? Galaxies with disks? How big are the disks (radius and thickness)? Do they have dark matter? Which galaxy does New Earth reside in? Can you produce an H-R diagram for New Earth's galaxy?</li> <li>Is the cosmological principle obeyed (i.e. homogeneity and isotropy?)</li> <li>What are the X-ray flashes all about? Why do they occur and how frequently?</li> <li>How massive are galaxies in your Universe? How many stars per galaxy (typically)? What are the temperatures and radii of these stars? What's their mass to light ratio?</li> <li>How far away are the galaxies in your Universe? More importantly: can you construct a convincing distance ladder?</li> <li>Can you determine if you have any groups or clusters of galaxies? What are their populations and stellar populations like? What is the ratio of total cluster mass to luminous cluster mass? Do the galaxies contain black holes?</li> <li>Compute Hubble's constant. How old is your Universe?</li> <li>Only for those of you feeling very ambitious!<ul> <li>How did your Universe begin and what is its future? Is there life elsewhere in your galaxy or Universe?</li> </ul> </li> </ul> <p>Much like investigating the real Universe, we don't expect you to address ALL of the above questions in a superbly detailed manner. You must pick and choose which questions or aspects of this project you want to concentrate on.</p> <p>However: We suggest that your starting point be constructing a distance ladder. You will get at least a 5 for an appropriately well written report if it contains a reasonably accurate determination of H0, with errorbars. This is your first objective! </p>"},{"location":"#deadlines-and-grading","title":"Deadlines and Grading","text":"<p>Please note that this is not an assignment that you can leave until the last possible moment. We will be spending a number of sessions in the computer lab for you to investigate the complex dataset, but you should also be continuing with this exercise in your own time. You have to do the following:</p> <ul> <li>Read the marking scheme here.</li> <li>Work in your groups to help each other solve the important parts of the problem.</li> <li>Document your contribution to the group work on the group discussion board. You must make at least one significant posting at least a week before the final deadline.</li> <li>Submit a full written report. This is your own report: you can use results from other group members (if acknowledged) but the writing must all be your own. It needs an abstract section that gives clear statements of your answers; see the marking scheme for more details. You may place detailed calculations in an appendix so as to not clutter up the body of the text. Please do not include code or lengthy data tables, even in an appendix. We will require an electronic copy of your assignment to be uploaded via Turnitin (accessed through Blackboard).</li> <li>In your project work and in the reports, concentrate on getting physical information out of the data. Avoid speculations that cannot be backed up by actual data. Also avoid drawing parallels to our universe. To the astronomers in New Universe, our universe is not available for comparison. Any physics you could know from the lab is safe, but any astrophysics could be completely different. </li> <li>Deadline for the written component? See Course Profile. Note the time deadline for electronic submission is strict and exact!</li> </ul>"},{"location":"About/","title":"About","text":""},{"location":"About/#contributors","title":"Contributors","text":"<p>These docs were prepared by Benjamin Pope (lecturer) and Ryan White (tutor).</p> <p>Data were generated by Ryan White.</p>"},{"location":"GitHub%20README/","title":"Simulated Universe","text":"<p>Preferably, use this link to the GitHub repository</p>"},{"location":"GitHub%20README/#about","title":"About","text":"<p>As part of the University of Queensland course PHYS3080 - Extragalactic Astrophysics and Cosmology, we students were given data from a simulated universe (programmed by the course staff) to create an astronomical distance ladder. Both the data analysis and the simulation itself fascinated me, so I'd decided to try my hand at emulating the simulation myself. While some aspects in this simulation aren't as polished as the data we were given as students, some new features have been added, and others extended. I'm really happy with the results so far, and I aim to work on it and tweak the code for a while to come. </p> <p>As python is the language I'm most capable in, this project is written entirely in python. Several packages were used extensively, including but not limited to: <code>numpy</code>, <code>matplotlib</code>, <code>pandas</code>, <code>multiprocessing</code>, and <code>scipy</code>.</p>"},{"location":"GitHub%20README/#how-to-use-the-program","title":"How to use the Program","text":"<p>Unless you intend to see the inner workings of the sim, the only file you would need to open is <code>MainSimulationFile.py</code>. To create a new dataset, just open this in your Python IDE of choice (or even just double click!), and run the program. This will create a new dataset in the \"Datasets\" folder, with a naming convention of \"Sim Data (Clusters; {pop}, Seed; {seed})\", where <code>pop</code> is the number of clusters and the <code>seed</code> parameter is just the seed for the random number generating in the program. Once the dataset is created, you can head over to the Datasets directory and take the .txt files within to analyse that universe!</p>"},{"location":"GitHub%20README/#extent-of-simulation","title":"Extent of Simulation","text":"<p>Several real aspects of astronomy are modelled (to varying degrees of accuracy) in this simulation. Below is a non-exhaustive list of the physical properties that could, in theory, be derived via quantitative and qualitative means from the output data <code>.txt</code> files. In the likely case that I can't follow that theme, the dot points show what and how some things were simulated.</p>"},{"location":"GitHub%20README/#stars","title":"Stars","text":"<ul> <li>Four distinct classes of stars are generated: Main Sequence, Giants, Supergiants, and White Dwarfs. They can be see on the HR diagram on the right in their usual positions.</li> <li>Main sequence stars are generated realistically according to luminosity-mass functions (with the mass first randomly generated via a gamma distribution), and then the radius generated according to mass-radius relations. The temperature is then solved via the Stefan-Boltzmann equation. I am quite happy with the behaviour of the MS stars!</li> <li>White dwarf mass distributions were generated according to a published paper on the topic. Radius was then generated by the \\(R \\sim M^{1/3}\\) relation and temperature derived from wikipedia data. The luminosity then was taken via the Stefan-Boltzmann law. </li> <li>The giant and supergiant parameter generation was far less realistic and was artificially chosen to give nice looking data on a HR diagram.</li> </ul> <ul> <li>Star emission is modelled accurately according to blackbody curves, with their temperature and luminosity impacting the shape and height of the curve as expected. As part of the data output, monochromatic luminosity data is exported for each star in wavelengths 440nm, 500nm and 700nm (roughly the B, V, and R bands respectively).</li> </ul> <ul> <li>Some populations of stars, depending on their position on the HR diagram, will have variable light curves. The simulation will determine whether to have two or three distinct variable populations, with a \"short\" (18-30 hour) period and a \"long\" (45-60 hour period) being certain. A \"longest\" (75-100 hour) period class of variable has a 1/3 chance of being generated too. Each of these classes of variables are shown as triangles in the HR diagram above, with the top population being the short period, the rightmost population being the longest period, and the bottom left being the long period. </li> <li>The lightcurve of variable stars is randomly chosen out of a Sine, Triangle, or Sawtooth wave, with some random scatter added onto the 'normalised flux'.</li> <li>The period-luminosity relation (linear trend on a logY-x scale) has a randomly assigned sign with a random gradient within some small range of the desired parameters. This is generated at the universe level in <code>Universe.py</code>.</li> </ul>"},{"location":"GitHub%20README/#galaxies","title":"Galaxies","text":"<ul> <li>Classification of galaxy types. cD, E0-7, S0 and Sa-c, SBa-c galaxy types are simulated to a reasonable degree of accuracy (but mostly to be a bit pretty and to break up monotony in galaxies). </li> <li>Different star populations are found throughout galaxies. In spirals, the bulge/bar has lower mass, redder stars. The leading edges of spirals have high mass blue stars, with lower mass redder stars trailing the leading edges. There are also a population of even lower mass stars (on average) not associated with spirals. Elliptical galaxies are characterised by many more, lower mass red stars.   </li> </ul> <p> (L) Black hole with no dark matter    (C) Black hole with dark matter    (R) No black hole with dark matter</p> <ul> <li>Rotation curves are simulated accurately according to newtonian physics, with black holes and dark matter (via the Navarro-Frenk-White profile) influencing rotation velocities. Spiral galaxies have coherent rotation, with all stars (except the central black hole cluster) orbiting in the orbital plane in the same direction. Elliptical galaxies have random velocity directions, but velocity magnitudes according to the same rotation curve profiles. This random velocity is really apparent in the doppler image below in the Galaxy Clusters section.</li> </ul> <p></p>"},{"location":"GitHub%20README/#black-holes","title":"Black Holes","text":"<ul> <li>Massive black holes (usually on the order of 50-500 solar masses) are generated at the center of all/most galaxies with the mass dependent on the galaxy mass as a whole. </li> <li>Black holes all have a luminosity on the order of the eddington luminosity for a black hole of that mass. The actual proportion of \\(L_\\text{edd}\\) depends on how many stars are in a dense cluster around the black hole.</li> <li>Mock 'Radio Lobes' from black hole accretion shoot out from the 'poles' of the galaxy (which assumes that the accretion disk is parallel with the plane of the galaxy). cD galaxies display Fanaroff-Riley Type I lobes (L), while other galaxies show FRII lobes (R). Lobes generate if the central BH has a luminosity of more than \\(10^6 L_\\odot\\). At the moment, there is no use for this other than qualitative means. </li> <li>Black holes have a cluster of massive stars around them (according to an exponential distribution, where ellipticals will have more stars), with random velocity directions. In the unlikely case you can actually see the black hole in an output image, it shows up as a dot with an aqua-ish colour (according to the blackbody colours link in the credits). </li> </ul>"},{"location":"GitHub%20README/#galaxy-clusters","title":"Galaxy Clusters","text":"<p>(L) A slightly more populous-than-average galaxy cluster.     (R) The radial velocities of galaxies within a cluster (red moving away).</p> <ul> <li>Galaxy clusters generate at least one galaxy according to an exponential distribution with a mean of 8 galaxies. Clusters with 5 or more galaxies will have an elliptical galaxy at their center, and clusters with 10 or more galaxies will have a cD galaxy in their center.</li> <li>Elliptical galaxies are much more common close to the center of clusters. Conversely, spirals are more likely on the outer edges of clusters. </li> <li>Using a similar approach to that of galaxy rotation curves, galaxies have a rotation velocity about the cluster center in a random direction (much similar to the method used for elliptical galaxies!)</li> </ul>"},{"location":"GitHub%20README/#the-universe-as-a-whole","title":"The Universe as a whole","text":"<ul> <li>Hubble recession is modelled, with more distant galaxies receeding further away. The hubble constant itself is chosen as a value between 1000 and 5000km/s/Mpc, with a random sign. </li> <li>Radial velocities of resolved stars take into account hubble recession, galaxy movement within clusters, and star movement within galaxies. </li> <li>In output visual images of the universe (see the top of this readme!), stars have diffraction spikes according to how bright they appear to the observer. Why? Because this is pretty and I like it. </li> <li>Type Ia supernova are randomly exploded across ~55 or so galaxies in the universe, with at least two of them being in the local cluster of galaxies so that the user may more easily find the intrinsic brightness. </li> <li>Homogeneous and inhomogeneous universes (inhomo by default) are able to be generated. Homogeneous has spherically-uniformly distributed galaxies, while inhomogeneous has exponentially increasing cluster counts up to a threshold of 30kpc (this is where normal galaxies are no longer generated and the 'distant galaxies' are now generated), where cluster count then exponentially decreases towards the radius of the universe. </li> </ul>"},{"location":"GitHub%20README/#to-do","title":"To-Do:","text":"<ul> <li> Make this damn README.md prettier!!</li> <li> Include a 'how to use' section in this readme (thanks saskia!)</li> <li> Fill out many docstrings and comment all code to a degree readable by other users.</li> </ul>"},{"location":"GitHub%20README/#creditsacknowledgements","title":"Credits/Acknowledgements","text":"<ul> <li>Saskia for providing a mental amount of help with regards to Python programming. I've learnt a lot! Also many many sanity checks and FEEDBACK. Mamma mia. </li> <li>Ciaran for helping with a bit of math (linear algebra is hard) and some astro sanity checks here and there. </li> <li>The <code>blackbodycolours.txt</code> file was supplied free of charge by Mitchell Charity (email) from What colour is a blackbody? (Version 2001-Jun-22)</li> <li>The astrophysics team at the University of Queensland for providing inspiration for this project in the form of their simulation data! </li> </ul>"},{"location":"License/","title":"License","text":"<p>MIT License</p> <p>Copyright (c) 2023 Ryan White</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"data/","title":"What's inside the data...","text":"<p>When opening the dataset folder, you'll be greeted by a few files all of which are relevant to working out properties of the universe within.</p>"},{"location":"data/#star_datacsv","title":"<code>Star_Data.csv</code>","text":"<p>Arguably the most important of the files are the star data files. Each resolved star in the universe has a line of data in these files. Measured is the stars position, its monochromatic (meaning one colour) luminosity for three wavelengths (using a highly sensitive instrument), parallax angle, radial velocity and a light curve (if the star is characterised by a variable luminosity). </p> Measurement Unit Explanation X degrees Horizontal position of the star in the image Y degrees Vertical position of the star in the image BlueF W/nm/m<sup>2</sup> Monochromatic luminosity of the star at 440nm GreenF W/nm/m<sup>2</sup> Monochromatic luminosity of the star at 500nm RedF W/nm/m<sup>2</sup> Monochromatic luminosity of the star at 700nm Parallax arcseconds Maximum parallax angle of the star across 1 year of observations RadialVelocity km/s Line-of-sight motion of the star (positive implies motion away) Variable? N/A 1 if the star shows variability in its luminosity, 0 otherwise."},{"location":"data/#variable-star-data","title":"<code>Variable Star Data</code>","text":"<p>Some stars in the universe will have a variable luminosity across time, and so this variability was measured if applicable and saved in a separate file for each star in the format <code>{starname}.csv</code>. The luminosity of the star was measured once every hour for 120 hours, and data is given in terms of a proportion of its average luminosity. </p> Measurement Unit Explanation Time hours The time (since starting) of the measurement NormalisedFlux N/A Proportion of baseline (average) luminosity"},{"location":"data/#distant_galaxy_datacsv","title":"<code>Distant_Galaxy_Data.csv</code>","text":"<p>Due to their small size in the sky, distant galaxies could be measured using the same instruments as those used to measure star properties. </p> Measurement Unit Explanation X degrees Horizontal position of the galaxy Y degrees Vertical position of the galaxy BlueF W/nm/m<sup>2</sup> Monochromatic luminosity of the galaxy at 440nm GreenF W/nm/m<sup>2</sup> Monochromatic luminosity of the galaxy at 500nm RedF W/nm/m<sup>2</sup> Monochromatic luminosity of the galaxy at 700nm Size arcseconds The width in the sky subtended by the galaxy RadialVelocity km/s Line-of-sight motion of the galaxy (positive implies motion away)"},{"location":"data/#flash_datacsv","title":"<code>Flash_Data.csv</code>","text":"<p>Across the period of observation (1 year), several extremely bright flashes were observed. The number of X-Ray photons observed were recorded using the all-sky monitor.</p> Measurement Unit Explanation Direction N/A The image face that the X/Y coordinates relate to X degrees Rough horizontal position of the bright flash on the specified image face Y degrees Rough vertical position of the bright flash on the specified image face Photon-Count N/A Number of X-Ray photons detected from the source"},{"location":"datasets/","title":"Datasets","text":"<p>Each group has a different Universe to work with, but note that they don't necessarily have the same properties!</p> <p>At the first lab, you were assigned into 10 groups. You are welcome to change if you feel a pressing need but if so please tell the lecturer immediately over email.</p> <p>Here are your datasets, in interactive webpages and in zip files you can download containing everything:</p> <ul> <li>Group 1: web, zip </li> <li>Group 2: web, zip </li> <li>Group 3: web, zip</li> <li>Group 4: web, zip</li> <li>Group 5: web, zip</li> <li>Group 6: web, zip</li> <li>Group 7: web, zip</li> <li>Group 8: web, zip</li> <li>Group 9: web, zip</li> <li>Group 10: web, zip</li> </ul>"},{"location":"datasets/#how-to-access-and-use-the-data","title":"How to Access and Use the Data","text":"<p>The first step in analysing the data is to get the data! Download one of the .zip files from above according to your group number and unzip it in a directory you'd like to work in. You can also use the html interface to navigate the data and pictures of your new universe!</p> <p>With these files downloaded, you can now start analysing the data. Taking a look at the Tutorials section may be a good spot to start!</p>"},{"location":"latex/","title":"Preparing a Report","text":"<p>You can use whichever software you like to write your report, so long as it follows the structure outlined above. But most scientists use the software LaTeX and we encourage you to as well; this is great practice for Honours, where your writing will all be required to be in LaTeX.</p> <p>In writing your report, please put your group number at the top, with your own name as first author and your colleagues as co-authors.</p>"},{"location":"latex/#using-latex","title":"Using LaTeX","text":"<p>Almost all scientific papers in astronomy are published using the typesetting software LaTeX, which not everyone may have used before. Here is a good guide to using LaTeX by the astronomy student collective astrobites. It is a markup language that allows you to include attractive figures and tables, mathematical notation, and easily keep track of citations with BibTeX. </p> <p>The easiest way to use LaTeX is via Overleaf, which is sort of like Google Docs for LaTeX, and allows you to share a document with your team and work collaboratively.</p> <p>Most of these papers are published in one of the major national journals, and there are Overleaf templates for each. In this report, you are required to use the template for the Australian national journal,  the Proceedings of the Astronomical Society of Australia (PASA).</p>"},{"location":"latex/#using-word","title":"Using Word","text":"<p>If you absolutely cannot use LaTeX, please ask. If necessary are allowed to use Microsoft Word, or another commercial word processor, but you should adhere to the same structure for a journal article as outlined above.</p>"},{"location":"latex/#example-reports","title":"Example Reports","text":"<p>Here are a few good example reports from previous years. Keep in mind that these reports may have been for a different type of universe and different version of code! </p> <ul> <li>Ryan White</li> </ul>"},{"location":"opensource/","title":"Open Source Software","text":"<p>When we say software is open-source, we mean that you can access the code, and are allowed to use it and modify it yourself under various light conditions. One of the great things about astronomy is that more than any other natural science we have a culture of open data, open-source software, and open access publishing - if you have a laptop and an internet connection, the barrier between you and looking at real cutting-edge astronomical data is very low. Part of the motivation for this course is to get you familiar with these tools and ideas, that you can use yourself in physics or any other subject or industry.</p>"},{"location":"opensource/#git-repositories","title":"Git Repositories","text":"<p>How do you share code with your colleagues? This is an important skill not just for this course - but anything you do down the track in science, software engineering, or the commercial sector. This doesn't just apply to Python scripts you'll write here, but to any sort of code - including this website.</p> <p>You want to have everything in a repository, which is a remote server in the cloud that has all your software backed up, with saved checkpoints you can go back to if something is wrong.</p> <p>The most popular software for interacting with repositories is <code>git</code>, and there are two big companies that offer comparable cloud storage for your repos: </p> <ul> <li>Atlassian offers BitBucket</li> <li>Microsoft runs GitHub</li> </ul> <p>You can share these codes with your teammates and jointly collaborate on a project. The user interface is a little harder than Google Docs but I recommend you master it - it is a huge transferable skill.  </p> <p>All my code is version-controlled github.com/benjaminpope. On my local machine I made a directory <code>/Users/benjaminpope/code/</code>, using terminal commands</p> <p><pre><code>cd .\nmkdir code\n</code></pre> and you can download a repo (like this website as an example!) to your machine like this:</p> <pre><code>cd code\ngit clone https://github.com/astrouq/ladder/\ncd ladder\n</code></pre> <p>GitHub also offers a GUI which may be a nice beginner-friendly experience if you've never used Git before! </p> <p>A really great way of organizing this project, from past experience, is if you create a repository of your own, shared with your group, and develop there. </p>"},{"location":"opensource/#making-open-source-software","title":"Making Open-Source Software","text":"<p>Christina Hedges (NASA Ames) has a fantastic introduction to open-source software practices for astronomy, in which the above tools and many others are explained, for an audience of ~ PhD students:</p> <ul> <li>christinahedges.github.io/astronomy_workflow</li> </ul>"},{"location":"report/","title":"Writing a Report","text":"<p>The assessment for the Distance Ladder project, and the follow-up Cosmology project, is based on writing a long-form report. For many of you this may be the longest you have done in a Physics major, but this is a very important skill - a practising scientist, in academia, government, or industry, spends typically more of their time reading and writing papers and proposals than doing calculations or working in the lab. A report is not a list of things you did; it is a story of how you came to see the world in a particular way, and why. The paragraphs and figures in the text should not just dump facts, but clearly lead us through this process.</p> <p>Reports should follow the PASA journal format if at all possible. They should contain a title (including your group number), with first-author being your name and coauthors your group colleagues, a clearly structured main body and either an abstract or a summary. Aim at writing 10 to 15 pages. Make sure figures are properly labeled, including legible and informative axis labels and units. Consider how to use space effectively on the page; use two-column figures where necessary, think about attractive use of colour to guide the eye. There is a light science-fiction setting in the course materials; you are allowed (and encouraged) to name things as you like, with as much whimsy or humour as you please, so long as the content is clear and understandable and the report is otherwise written like a scientific paper. Do not include lengthy data tables or source code in your report. You may previously have been taught, for example at school, that science is written in the passive voice ('the data were processed... the figures were generated with...'). This is no longer common practice: you want to write \"We processed the data... we generated figures with...\" etc. </p> <p>In brief - you will get a 5 if you have a reasonably well written report across all marking criteria, with the only essential science content being a determination of the Hubble constant. Higher marks will be awarded for better-written reports that determine more physics about the universe.</p> <p>You will be guaranteed to get feedback on a draft report if you send it to Ben by 7 days before the deadline. If you send a draft later than that, he will do his best to get to you but may not have the opportunity.</p>"},{"location":"report/#structure-of-a-scientific-report","title":"Structure of a Scientific Report","text":"<p>You are required to follow the standard structure for scientific writing in your report, just as if you were writing a real paper. Writing papers like this is one of the main things you are expected to do in real-world science, and a good practice for your Honours thesis.</p> <p>Structure:</p> <ul> <li>Title: pick a punchy title that accurately encapsulates what you did. You can include your group number here, or under Affiliations.</li> <li>Authors and Affiliations: here you list authors and affiliations (ie which university or institute each person is from; for you this will all be UQ, but most science is done by teams spread across many institutes). In astronomy the first author is the main person responsible, but in some fields of science this is the last author for historical reasons. In your report you should put your own name first, and then list all your team members.</li> <li>Abstract: 200 words summarizing what you did. Every day hundreds of papers are published, and scientists mainly skim the title and abstract before deciding whether to read a paper. I like to follow a condensed form of the paper itself: start with<ul> <li>state the overall topic </li> <li>what is a problem with our current understanding</li> <li>state your new idea, data, or technique that you can use to solve this problem</li> <li>state your results</li> <li>briefly summarize the results' relevance to the overall topic</li> </ul> </li> <li>Introduction: Give the reader the context they need to understand your paper. Not just a brain dump, but state how it fits into their knowledge: in order, say what is the topic, what do we not understand, and why write this paper now. Science is an ongoing international conversation about the natural world, and people want to know where in this conversation your work fits. There is typically a new idea, dataset, technique, invention, or something else that has motivated a new contribution to the literature. </li> <li>Methods: You can break this up into subsections if you like. This is where you summarize what is in the data, and how you analysed it (eg with what software). Do not include raw code; if you want to link to your software, you can include a link to a GitHub repository. </li> <li>Results: What did you find? You should include subsections for the Hubble constant, and anything you did about period-luminosity relations, stellar populations, galaxies, the structure of the universe, et cetera. </li> <li>Discussion: How do you interpret your results? This can be long or brief as you please.</li> <li>Conclusions: It can be hard to get the hang of writing a conclusions section - how does it differ from Discussion? The difference is that in Conclusions you talk more about the relevance of your paper to the broad questions addressed in the abstract and introduction, and to the </li> <li>Appendices: You don't need appendices, and you probably shouldn't have any. Your report definitely shouldn't include any large tables of raw data or code, and marks will be deducted if you include any. But if you have tables of reduced data like stellar temperatures or galaxy masses, or long calculations, this is where this would go.</li> </ul> <p>Here are some examples of papers by my team and colleagues that you might like to follow for structure:</p> <ul> <li>\"Modelling Cosmic Radiation Events in the Tree-ring Radiocarbon Record\", Zhang et al, RSPA, 2022</li> <li>\"The TESS View of LOFAR Radio-Emitting Stars\", Pope et al, ApJL, 2021</li> <li>\"Aldebaran b's temperate past uncovered in planet search data\", Farr et al, ApJL, 2018</li> </ul>"},{"location":"software/","title":"Python","text":""},{"location":"software/#the-command-line","title":"The command line","text":"<p>You'll want to have access to a computer with a command line interface - on Mac or Linux, this is just going to be via the app Terminal. On Windows this is called the Command Prompt. (If you have trouble installing you can also use the Google Colab I will mention later!)</p>"},{"location":"software/#installing-python","title":"Installing Python","text":"<p>Most software development will be done in Python, and I recommend using Anaconda to install Python 3 and pip to manage packages. What it does is create its own version of Python that doesn't interfere with your default install, and has 'environments' into which you can install software safely that doesn't interact with software in other environments.</p> <p>There is a great conda cheat sheet with lots of tools to help you use Conda.</p> <p>Anaconda is available on the UQ Digital Workspaces as the package <code>Anaconda3-2020.02</code>, although this is much more tedious to use than installing Anaconda on your own machine. </p> <p>To install Python on a Windows/Mac/Linux machine of your own, I recommend you install Conda from here.</p> <p>When working in Python, it is best to create a new environment for each project. For this project, you will want these important packages pre-installed:</p> <ul> <li><code>pip</code>, which installs other Python packages,</li> <li><code>numpy</code>, which is a general-purpose maths library,</li> <li><code>matplotlib</code>, the general-purpose Python plotting library,</li> <li><code>astropy</code>, which has lots of functions for astronomy, including the Lomb-Scargle Periodogram for period determination,</li> <li><code>pandas</code>, which is a useful tool for loading and working with data,</li> <li><code>scipy</code>, with miscellaneous scientific Python features, and</li> <li><code>ipykernel</code>, which runs Jupyter notebooks.</li> </ul> <p>You can create a new environment called <code>ladder</code> with all of this, using the following terminal commands. </p> <p>First set up conda (only do this once):</p> <p><pre><code>conda init\n</code></pre> then in your terminal, create an environment called <code>ladder</code>, with some default software installed:</p> <pre><code>conda create --name ladder python=3.10 pip numpy matplotlib astropy scipy ipykernel pandas tqdm\n</code></pre> <p>Then you want to activate this ennvironment, and install the Jupyter notebook packages.</p> <pre><code>conda activate ladder\nconda install -c conda-forge notebook\nconda install -c conda-forge jupyterlab\nconda install -c conda-forge nb_conda_kernels\n</code></pre> <p>Then you can work to your heart's content in this conda environment. </p>"},{"location":"software/#python-in-windows","title":"Python in Windows","text":"<p>Most software development is done in Unix-based operating systems, the main examples being the Linux distributions, and Apple's Mac OSX. For a long time it has been a bit of a hassle to use the latest tools in Windows, but things have got a lot better recently!</p> <p>Anaconda (including Jupyter Notebook and their IDE Spyder) should work out of the box on Windows. The majority of popular python packages are built with Windows use in mind (including all of those we recommend using for this project!) although some niche packages may require a Unix-based system. If this is the case you should use Windows Subsystem for Linux, which runs a Linux distribution (by default Ubuntu, which is my favourite too) inside of Windows. You should be able to do basically everything I suggested on the command line through this. By default, on Windows 10 or later, you should be able to open a PowerShell command line and type</p> <pre><code>wsl --install\n</code></pre> <p>and it should install; if this doesn't work, there are more detailed instructions here.</p>"},{"location":"software/#using-python","title":"Using Python","text":"<p>There is a great intro textbook for Python for astronomers, freely available online here by Pasha &amp; Agostino.  Chapter 1 isn't particularly relevant for Windows use of Anaconda, but will be a great resource when doing later research in a capstone/honours! </p>"},{"location":"software/#jupyter-notebooks","title":"Jupyter Notebooks","text":"<p>The main way that professional data scientists, physicists, and astronomers interactively use Python is through the Jupyter Notebook environment. This is an interactive, browser-based interpreter for Python.</p> <p>Here is a great tutorial on using Jupyter notebooks - if you haven't before, I recommend you try this! </p> <p>If you install Conda like above, you'll be good to go with the terminal command <code>jupyter notebook</code> or by searching for the Jupyter Notebook app (Windows).</p>"},{"location":"software/#google-colab","title":"Google Colab","text":"<p>While most users will have Conda working very nicely, some users have difficulty installing (and Chromebooks are incompatible!).</p> <p>If this is the case for you, I recommend you use Google Colab, a free web-hosted Jupyter notebook environment that works like Google Docs. (There are paid versions, but you will not need to use these right now.)</p> <p>Try it here! You can upload your project data to your Google drive, and instead of using <code>conda</code> to manage your environment and installed packages, in the first cell just execute</p> <pre><code>! pip install numpy matplotlib astropy scipy ipykernel\n</code></pre> <p>I have created Google Colab versions of the old tutorial pages from this site. Don't just copy and paste the code, but try to read this as a resource for how to use Colab: </p> <ul> <li>HR Diagram</li> <li>Lomb-Scargle Periodogram</li> </ul> <p>The important thing to remember is you will have to host your project data on your Google Drive or GitHub and download it when you start your Colab session.</p>"},{"location":"tutorials/Uncertainties/","title":"Uncertainties in Data","text":"<p>Due to the nature of taking measurements, all measurements have an inherent uncertainty in their true values. </p>"},{"location":"tutorials/Uncertainties/#variable-star-data","title":"Variable Star Data","text":"<p>Each <code>.txt</code> file within this directory has measurement uncertainty corresponding to</p> Measurement Uncertainty Unit Time +/- 0.3 hours NormalisedFlux +/- 1.5 %"},{"location":"tutorials/Uncertainties/#distant-galaxy-datatxt","title":"<code>Distant Galaxy Data.txt</code>","text":"Measurement Uncertainty Unit Equatorial +/- 0.0001 degrees Polar +/- 0.0001 degrees BlueF +/- 1 % GreenF +/- 1 % RedF +/- 1 % Size +/- 10 % RadialVelocity +/- 0.1 km/s"},{"location":"tutorials/Uncertainties/#radio-source-datatxt","title":"<code>Radio Source Data.txt</code>","text":"Measurement Uncertainty Unit Equatorial +/- 0.01 degrees Polar +/- 0.01 degrees Luminosity +/- 10 %"},{"location":"tutorials/Uncertainties/#star-datatxt","title":"<code>Star Data.txt</code>","text":"Measurement Uncertainty Unit Equatorial +/- 0.0001 degrees Polar +/- 0.0001 degrees BlueF +/- 1 % GreenF +/- 1 % RedF +/- 1 % Parallax +/- 0.001 arcseconds RadialVelocity +/- 0.03 km/s Variable? N/A N/A"},{"location":"tutorials/Uncertainties/#supernova-datatxt","title":"<code>Supernova Data.txt</code>","text":"Measurement Uncertainty Unit Equatorial +/- 0.01 degrees Polar +/- 0.01 degrees PeakFlux(W) +/- 1 %"},{"location":"tutorials/UsefulFormulae/","title":"UsefulFormulae","text":"<p>Unless said otherwise, assume all units used in the equations below require values in SI units! </p>"},{"location":"tutorials/UsefulFormulae/#useful-constants","title":"Useful Constants","text":"Name Symbol Value Unit Speed of light \\(c\\) \\(299792458\\) m/s Meters to parsec conversion N/A \\(3.086 \\times 10^{16}\\) m/pc Solar mass \\(M_\\odot\\) \\(1.98 \\times 10^{30}\\) kg Solar luminosity \\(L_\\odot\\) \\(3.828 \\times 10^{26}\\) W Solar absolute magntiude (bolometric) \\(M_\\odot\\) 4.83 N/A Planck's constant \\(h\\) \\(6.626 \\times 10^{-34}\\) J.s Boltzmann's constant \\(k_B\\) \\(1.38 \\times 10^{-23}\\) m\\(^2\\).kg.s\\(^{-2}\\).K\\(^{-1}\\) Stefan-Boltzmann constant \\(\\sigma\\) \\(5.67 \\times 10^{-8}\\) W.m\\(^{-2}\\).K\\(^{-4}\\)"},{"location":"tutorials/UsefulFormulae/#general-formulae","title":"General Formulae","text":"<p>For some equatorial angle \\(\\theta\\) and polar angle \\(\\phi\\), generating cartesian coordinates from spherical coordinates is done by</p> \\[ x =  r \\cos (\\theta) \\sin (\\phi) \\] \\[ y = r \\sin (\\theta) \\sin (\\phi) \\] \\[ z = r \\cos (\\phi) \\] <p>To find the distance to an object from a parallax measurement, \\(p\\) (with their angles in units of arcseconds!), </p> \\[ d = \\frac{1}{p\"} \\text{ pc} \\]"},{"location":"tutorials/UsefulFormulae/#stars","title":"Stars","text":""},{"location":"tutorials/UsefulFormulae/#luminosity-and-flux","title":"Luminosity and Flux","text":"<p>The flux and luminosity of a star are related when considering their distance from an observer by </p> \\[ F = \\frac{L}{4 \\pi r^2} \\] <p>The bolometric luminosity of a star is related to its radius and temperature via the Stefan-Boltzmann law, </p> \\[ L_\\text{bol} = 4 \\pi R^2 \\sigma T^4 \\] <p>While not explicitly required at all in analysis of this data, you may choose to work with fluxes in terms of the magnitude scale. The connection between distance, \\(d\\), apparent magnitude \\(m\\), and absolute magnitude, \\(M\\) of a star is given by </p> \\[ d = 10^{(m - M + 5) / 5} \\text{ pc} \\] <p>The luminosity ratio of two stars is then </p> \\[ \\frac{L_2}{L_1} = 100^{(M_1 - M_2) / 5} \\] <p>and so, in terms of solar luminosities and magnitude, </p> \\[ M = M_\\odot - 2.5 \\log_{10} \\left(\\frac{L}{L_\\odot}\\right) \\]"},{"location":"tutorials/UsefulFormulae/#variability","title":"Variability","text":"<p>The variability of stars follows a consistent wave (that is to say, it's periodicity doesn't change over time). Most waveforms can be approximated as sinusoidal waves, with function</p> \\[ f(x) = A \\sin \\left(\\frac{2\\pi}{P}(x - S)\\right) + E \\] <p>While not all waves might follow a nice sine function, their periodicity can usually be approximated as one so it's a useful tool!</p>"},{"location":"tutorials/UsefulFormulae/#spectra","title":"Spectra","text":"<p>The spectrum of stars follows (pretty closely) a Planck function for a blackbody. This is given below, for a wavelength \\(\\lambda\\) (meters), temperature \\(T\\) (K), and constants \\(h\\) (Planck's constant), \\(c\\) (speed of light), and \\(k_B\\) (Boltzmann's constant).</p> \\[ B_\\lambda (\\lambda, T) = \\frac{2hc^2}{\\lambda^5} \\frac{1}{\\text{exp}\\left(\\frac{hc}{\\lambda k_B T}\\right) - 1} \\] <p>This on its own isn't very helpful, but you can use it to find the monochromatic luminosity of a star (its luminosity in only one colour).</p> \\[  L_\\lambda = 4\\pi^2 R^2 B_\\lambda \\] <p>The location of the peak of the blackbody curve for a star is related to its temperature by</p> \\[ \\lambda_\\text{max} T \\approx 0.29 \\text{ cm K} \\] <p>Source: http://burro.cwru.edu/academics/Astr221/Light/blackbody.html</p>"},{"location":"tutorials/UsefulFormulae/#black-holes","title":"Black Holes","text":"<p>The (Schwarzschild) radius of a black hole is related to its mass by </p> \\[ r_S = \\frac{2GM}{c^2} \\] <p>The eddington luminosity of a black hole is approximated by </p> \\[L_\\text{edd} \\approx 3 \\times 10^4 M\\] <p>where \\(M\\) is in terms of the solar masses of the black hole, and the output value of \\(L_\\text{edd}\\) is in terms of solar luminosities.</p>"},{"location":"tutorials/UsefulFormulae/#galaxies","title":"Galaxies","text":""},{"location":"tutorials/UsefulFormulae/#rotational-velocities","title":"Rotational Velocities","text":"<p>The magnitude of the orbital velocity of a star about the center of a galaxy can be approximated by</p> \\[ v = \\sqrt{\\frac{G M(&lt;R)}{R}} \\] <p>where \\(M(&lt;R)\\) represents all of the mass enclosed within the orbital radius of the star, and so \\(R\\) is then obviously the orbital radius of the star. \\(G\\) here is the gravitational constant. That mass term may not include only visible matter, and some dark matter may be present too. In the case of dark matter (you might be able to infer its presence by the shape of the rotation curve!), the Navarro-Frenk-White (NFW) profile of dark matter (which models an isotropic but density-changing mass distribution of dark matter) must be accounted for. </p>"},{"location":"tutorials/UsefulFormulae/#clusters","title":"Clusters","text":"<p>The above section for rotational velocities holds in the context of galaxy clusters too, practically identically (except instead of dealing with stars, we're dealing with galaxies as a whole). </p>"},{"location":"tutorials/UsefulFormulae/#universe","title":"Universe","text":"<p>The velocity of distant objects due to Hubble recession is related to Hubble's constant (\\(H_0\\)) and their distance away (\\(D\\)) by </p> \\[ v = H_0 D \\] <p>Pay careful attention to unit conversions with this formula! </p>"},{"location":"tutorials/fitting-a-line/","title":"Fitting a Line to Data","text":"In\u00a0[70]: Copied! <pre>import numpy as np # for maths \nimport matplotlib as mpl # for plotting \nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm # tqdm is a package that lets you make progress bars to see how a loop is going\n\nimport os \n\n# configure notebook for plotting\n%matplotlib inline\n\nmpl.style.use('seaborn-colorblind') # colourblind-friendly colour scheme\n\n# subsequent lines default plot settings\nmpl.rcParams['image.origin'] = 'lower'\nmpl.rcParams['figure.figsize']=(8.0,6.0)   \nmpl.rcParams['font.size']=16              \nmpl.rcParams['savefig.dpi']= 300             \n\nimport warnings\nwarnings.filterwarnings('ignore')\n</pre> import numpy as np # for maths  import matplotlib as mpl # for plotting  import matplotlib.pyplot as plt  from tqdm import tqdm # tqdm is a package that lets you make progress bars to see how a loop is going  import os   # configure notebook for plotting %matplotlib inline  mpl.style.use('seaborn-colorblind') # colourblind-friendly colour scheme  # subsequent lines default plot settings mpl.rcParams['image.origin'] = 'lower' mpl.rcParams['figure.figsize']=(8.0,6.0)    mpl.rcParams['font.size']=16               mpl.rcParams['savefig.dpi']= 300               import warnings warnings.filterwarnings('ignore') <p>First, let's generate some fake data, from the line <code>y = 12*x - 5</code> for <code>x</code> in (0,2.5), with Gaussian noise of \u03c3=2.</p> In\u00a0[71]: Copied! <pre>x = np.linspace(0,2.5,100)\nnoise = 2*np.random.randn(len(x))\ny = 12*x -5  + noise\n\nplt.plot(x,12*x-5,'-k',label='True')\nplt.plot(x,y,'.',label='Data')\n\nplt.legend()\n</pre> x = np.linspace(0,2.5,100) noise = 2*np.random.randn(len(x)) y = 12*x -5  + noise  plt.plot(x,12*x-5,'-k',label='True') plt.plot(x,y,'.',label='Data')  plt.legend() Out[71]: <pre>&lt;matplotlib.legend.Legend at 0x7fe99c885760&gt;</pre> <p>To fit a straight line to these data, we can easily do this by least squares - ie, minimizing the sum of the squares of the error: <code>(model-data)**2</code>.</p> <p>We can express our problem as <code>y = A @ b</code>, for a 2xN 'design matrix' <code>A</code> whose first column is <code>x</code> and second column is all <code>1</code>, and a 'coefficient vector' <code>b</code>.</p> <p>This has an easy solution by computational linear algebra, implemented as <code>np.linalg.lstsq</code>.</p> In\u00a0[72]: Copied! <pre>A = np.vander(x,2) # the Vandermonde matrix of order N is the matrix of polynomials of an input vector 1, x, x**2, etc\n\nb, residuals, rank, s = np.linalg.lstsq(A,y)\nprint('True parameters: 12, -5. Recovered parameters: %.2f, %.2f' % (b[0],b[1]))\n\nreconstructed = A @ b # @ is shorthand for matrix multiplication in python\n\nplt.plot(x,y,'.',label='Data')\nplt.plot(x,reconstructed,'-r',label='Reconstructed')\nplt.plot(x,12*x-5,'-k',label='True')\nplt.legend()\n</pre> A = np.vander(x,2) # the Vandermonde matrix of order N is the matrix of polynomials of an input vector 1, x, x**2, etc  b, residuals, rank, s = np.linalg.lstsq(A,y) print('True parameters: 12, -5. Recovered parameters: %.2f, %.2f' % (b[0],b[1]))  reconstructed = A @ b # @ is shorthand for matrix multiplication in python  plt.plot(x,y,'.',label='Data') plt.plot(x,reconstructed,'-r',label='Reconstructed') plt.plot(x,12*x-5,'-k',label='True') plt.legend() <pre>True parameters: 12, -5. Recovered parameters: 11.65, -4.44\n</pre> Out[72]: <pre>&lt;matplotlib.legend.Legend at 0x7fe99c8d5af0&gt;</pre> <p>So far so good! But science is about uncertainties. How can we estimate the uncertainties on the slope and intercept?</p> <p>Enter the Monte Carlo method - named for a famous casino, Monte Carlo is the idea of using random number generation to solve problems in computer science.</p> <p>Here, what we can do, is simulate many datasets with the same noise as the data we have, and fit lines to those, to estimate the range of possible lines that fit our data.</p> In\u00a0[73]: Copied! <pre>bs = np.zeros((2,1000))\n\nfig, (ax1, ax2) = plt.subplots(1,2,figsize=(14.0,6.0))\n\nax1.plot(x,y,'.')\n\nfor j in tqdm(range(1000)):\n    ydash = y + 2*np.random.randn(len(x)) # generate random samples\n    bdash, _, _, _ = np.linalg.lstsq(A,ydash) # for each sample, redo the solve\n    reconstructed = A @ bdash # @ is shorthand for matrix multiplication in python\n\n    bs[:,j] = bdash\n    ax1.plot(x,reconstructed,'-r',alpha=0.01) # plot with low alpha = transparent, great way to show density of many curves\n\nax1.plot(x,12*x-5,'-k')\n\nax2.hist(bs[0],bins=25,label='Estimates'); # 0 is slope, 1 is intercept\nax2.axvline(12.0,color='k',linestyle='--',label='True')\nax2.legend()\n</pre> bs = np.zeros((2,1000))  fig, (ax1, ax2) = plt.subplots(1,2,figsize=(14.0,6.0))  ax1.plot(x,y,'.')  for j in tqdm(range(1000)):     ydash = y + 2*np.random.randn(len(x)) # generate random samples     bdash, _, _, _ = np.linalg.lstsq(A,ydash) # for each sample, redo the solve     reconstructed = A @ bdash # @ is shorthand for matrix multiplication in python      bs[:,j] = bdash     ax1.plot(x,reconstructed,'-r',alpha=0.01) # plot with low alpha = transparent, great way to show density of many curves  ax1.plot(x,12*x-5,'-k')  ax2.hist(bs[0],bins=25,label='Estimates'); # 0 is slope, 1 is intercept ax2.axvline(12.0,color='k',linestyle='--',label='True') ax2.legend() <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:01&lt;00:00, 819.75it/s]\n</pre> Out[73]: <pre>&lt;matplotlib.legend.Legend at 0x7fe99c8d63a0&gt;</pre> <p>Great! We can now quantify the uncertainty in our estimates of the slope.</p> <p>For you to think about:</p> <ul> <li>How would you fit a line if there are uncertainties in both <code>x</code> and <code>y</code>?</li> <li>How could you use Monte Carlo to estimate uncertainties in a Lomb-Scargle?</li> <li>Our fitting isn't ideal if your uncertainties aren't exactly iid Gaussian. Try looking up Markov Chain Monte Carlo, which is the best way to fit any model to any data and get uncertainties in your model parameters. <ul> <li>Code tutorial on MCMC with Python package <code>emcee</code> by Dan Foreman-Mackey </li> <li>Theory paper explaining why you do this by Hogg et al: arXiv:1008.4686</li> </ul> </li> </ul>"},{"location":"tutorials/fitting-a-line/#fitting-a-line-to-data","title":"Fitting a Line to Data\u00b6","text":"<p>In this tutorial, we'll go through how to fit a straight line <code>y = m*x + b</code> to data. This will come up a lot in this project, and in the rest of your life!</p>"},{"location":"tutorials/hr_diagram/","title":"HR Diagram","text":"<p>First, we'll load all the Python packages we need.</p> In\u00a0[1]: Copied! <pre>import numpy as np # for maths \nimport matplotlib # for plotting \nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nimport os \n\nimport pandas as pd # pandas is a popular library in industry for manipulating large data tables\n\n# configure notebook for plotting\n%matplotlib inline \nmpl.style.use('seaborn-colorblind') # colourblind-friendly colour scheme\n\n# define default plot settings\nmatplotlib.rcParams['image.origin'] = 'lower'\nmatplotlib.rcParams['figure.figsize']=(8.0,6.0)    #(6.0,4.0)\nmatplotlib.rcParams['font.size']=16              #10 \nmatplotlib.rcParams['savefig.dpi']= 300             #72 \n\nimport warnings\nwarnings.filterwarnings('ignore')\n</pre> import numpy as np # for maths  import matplotlib # for plotting  import matplotlib as mpl import matplotlib.pyplot as plt  import os   import pandas as pd # pandas is a popular library in industry for manipulating large data tables  # configure notebook for plotting %matplotlib inline  mpl.style.use('seaborn-colorblind') # colourblind-friendly colour scheme  # define default plot settings matplotlib.rcParams['image.origin'] = 'lower' matplotlib.rcParams['figure.figsize']=(8.0,6.0)    #(6.0,4.0) matplotlib.rcParams['font.size']=16              #10  matplotlib.rcParams['savefig.dpi']= 300             #72   import warnings warnings.filterwarnings('ignore') <pre>/var/folders/q3/wlh9qxyn1nxdm7v7l30qryf00000gn/T/ipykernel_1752/2746787847.py:12: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-&lt;style&gt;'. Alternatively, directly use the seaborn API instead.\n  mpl.style.use('seaborn-colorblind') # colourblind-friendly colour scheme\n</pre> In\u00a0[7]: Copied! <pre>ddir = '/Users/benjaminpope/Downloads/universe_6683/' # this should be where you unzipped your data\n</pre> ddir = '/Users/benjaminpope/Downloads/universe_6683/' # this should be where you unzipped your data In\u00a0[11]: Copied! <pre>stars = pd.read_csv(ddir+'Top/Star_Data.csv') \nprint(stars.keys()) # this tells us what column names we have\n</pre> stars = pd.read_csv(ddir+'Top/Star_Data.csv')  print(stars.keys()) # this tells us what column names we have <pre>Index(['Name', 'X', 'Y', 'BlueF', 'GreenF', 'RedF', 'Parallax',\n       'RadialVelocity', 'Variable?'],\n      dtype='object')\n</pre> <p>It looks like we have a bunch of clusters of galaxies here! Space is big!</p> In\u00a0[12]: Copied! <pre>plt.scatter(stars.X,stars.Y)\nplt.xlabel('x (pix)')\nplt.ylabel('y (pix)');\n</pre> plt.scatter(stars.X,stars.Y) plt.xlabel('x (pix)') plt.ylabel('y (pix)'); <p>So let's focus in on one of them in the top right:</p> In\u00a0[21]: Copied! <pre>centre = (43.3, 35.58)\nplt.scatter(stars.X,stars.Y)\nplt.scatter(*centre,color='C2',marker='X') # * expands the elements of a list \nplt.xlabel('x (pix)')\nplt.ylabel('y (pix)');\n</pre> centre = (43.3, 35.58) plt.scatter(stars.X,stars.Y) plt.scatter(*centre,color='C2',marker='X') # * expands the elements of a list  plt.xlabel('x (pix)') plt.ylabel('y (pix)'); In\u00a0[24]: Copied! <pre>d = np.sqrt((stars.X-centre[0])** 2 + (stars.Y - centre[1])**2)\ngalaxy = stars[d&lt;1] # filter to only close ones\nplt.scatter(galaxy.X,galaxy.Y,c=galaxy.RadialVelocity,cmap=mpl.cm.seismic) # let's overplot the radial velocities\nplt.colorbar()\nplt.scatter(*centre,color='C2',marker='X') # * expands the elements of a list \nplt.xlabel('x (pix)')\nplt.ylabel('y (pix)');\n</pre> d = np.sqrt((stars.X-centre[0])** 2 + (stars.Y - centre[1])**2) galaxy = stars[d&lt;1] # filter to only close ones plt.scatter(galaxy.X,galaxy.Y,c=galaxy.RadialVelocity,cmap=mpl.cm.seismic) # let's overplot the radial velocities plt.colorbar() plt.scatter(*centre,color='C2',marker='X') # * expands the elements of a list  plt.xlabel('x (pix)') plt.ylabel('y (pix)'); <p>Wow! A clearly rotating spiral galaxy!</p> <p>Let's make an HR diagram. One thing to keep in mind is that our convention of using magnitudes is just to match our modern scale to the Ancient Greeks - in your fantasy universe, you can use whatever units you like (but I recommend that they be logarithmic!). This is an important note - lots of students get hung up on the -2.5 and the solar luminosity, but these are just conventions in our world that aren't important for the physics.</p> <p>So let's have a simple log scale for magnitude here.</p> In\u00a0[25]: Copied! <pre>m0, m1, m2 = (np.log10(galaxy['BlueF']), \n              np.log10(galaxy['GreenF']), \n              np.log10(galaxy['RedF'])) \ncolour = m2-m0\n</pre> m0, m1, m2 = (np.log10(galaxy['BlueF']),                np.log10(galaxy['GreenF']),                np.log10(galaxy['RedF']))  colour = m2-m0 In\u00a0[26]: Copied! <pre>s = plt.scatter(colour,m1)\nplt.ylabel('Log Flux 1')\nplt.xlabel('Log Flux 2 - Log Flux 0')\n</pre> s = plt.scatter(colour,m1) plt.ylabel('Log Flux 1') plt.xlabel('Log Flux 2 - Log Flux 0') Out[26]: <pre>Text(0.5, 0, 'Log Flux 2 - Log Flux 0')</pre> <p>Great! We see a main sequence and a giant branch. Let's look at the parallaxes and see if we can get an absolute distance...</p> In\u00a0[27]: Copied! <pre>print('Parallaxes: mean %.3f, sd %.3f' % (np.mean(galaxy['Parallax']),np.std(galaxy['Parallax'])))\n</pre> print('Parallaxes: mean %.3f, sd %.3f' % (np.mean(galaxy['Parallax']),np.std(galaxy['Parallax']))) <pre>Parallaxes: mean 0.000, sd 0.001\n</pre> <p>Oh dear. Look at those parallaxes - tiny, some are even negative (this is just noise, and happens in real data!). Useless for an absolute distance determination!</p> In\u00a0[28]: Copied! <pre>import glob # this package lets you search for filenames\nimport os\n\nall_stars = glob.glob(ddir+'*/Star_Data.csv')\n</pre> import glob # this package lets you search for filenames import os  all_stars = glob.glob(ddir+'*/Star_Data.csv') In\u00a0[29]: Copied! <pre>fig, ax1 = plt.subplots(1,1)\nfor j, catalog in enumerate(all_stars):\n    try:\n        this = pd.read_csv(catalog)\n        \n        thispar = this.Parallax\n        thism0, thism1, thism2 = (np.log10(this.BlueF), \n                                  np.log10(this.GreenF), \n                                  np.log10(this.RedF))\n        thiscolour = thism2-thism0\n        dist = 1/thispar\n        abs_mag = thism1 + 2*np.log10(dist) \n        mm = thispar&gt;0.010 # only pick the ones with good signal-to-noise - 10 mas is ok \n        \n        ax1.scatter(thiscolour[mm],abs_mag[mm],color='C1')\n    except:\n        pass\n\nplt.ylabel('Log Flux 1')\nplt.xlabel('Log Flux 2 - Log Flux 0')\n</pre> fig, ax1 = plt.subplots(1,1) for j, catalog in enumerate(all_stars):     try:         this = pd.read_csv(catalog)                  thispar = this.Parallax         thism0, thism1, thism2 = (np.log10(this.BlueF),                                    np.log10(this.GreenF),                                    np.log10(this.RedF))         thiscolour = thism2-thism0         dist = 1/thispar         abs_mag = thism1 + 2*np.log10(dist)          mm = thispar&gt;0.010 # only pick the ones with good signal-to-noise - 10 mas is ok                   ax1.scatter(thiscolour[mm],abs_mag[mm],color='C1')     except:         pass  plt.ylabel('Log Flux 1') plt.xlabel('Log Flux 2 - Log Flux 0') Out[29]: <pre>Text(0.5, 0, 'Log Flux 2 - Log Flux 0')</pre> <p>Cool! There are tons of stars in here!</p> In\u00a0[30]: Copied! <pre>fig = plt.figure()\n\nfor j, catalog in enumerate(all_stars):\n    try:\n        this = pd.read_csv(catalog)\n        \n        thispar = this.Parallax\n        thism0, thism1, thism2 = (np.log10(this.BlueF), \n                                  np.log10(this.GreenF), \n                                  np.log10(this.RedF))\n        thiscolour = thism2-thism0\n        dist = 1/thispar\n        abs_mag = thism1 + 2*np.log10(dist) \n        mm = thispar&gt;0.010 # only pick the ones with good signal-to-noise - 10 mas is ok \n        \n        h = plt.scatter(thiscolour[mm],abs_mag[mm],color='C1')\n    except:\n        pass\n\ns = plt.scatter(colour,m1,color='C0')\n\nplt.ylabel('Log Flux 1')\nplt.xlabel('Log Flux 2 - Log Flux 0')\n\nplt.legend([h,s],['Benchmark','Cluster'])\n</pre> fig = plt.figure()  for j, catalog in enumerate(all_stars):     try:         this = pd.read_csv(catalog)                  thispar = this.Parallax         thism0, thism1, thism2 = (np.log10(this.BlueF),                                    np.log10(this.GreenF),                                    np.log10(this.RedF))         thiscolour = thism2-thism0         dist = 1/thispar         abs_mag = thism1 + 2*np.log10(dist)          mm = thispar&gt;0.010 # only pick the ones with good signal-to-noise - 10 mas is ok                   h = plt.scatter(thiscolour[mm],abs_mag[mm],color='C1')     except:         pass  s = plt.scatter(colour,m1,color='C0')  plt.ylabel('Log Flux 1') plt.xlabel('Log Flux 2 - Log Flux 0')  plt.legend([h,s],['Benchmark','Cluster']) Out[30]: <pre>&lt;matplotlib.legend.Legend at 0x7ff3536b9240&gt;</pre> <p>So this cluster is much further away! By trial and error, let's find a distance modulus that looks right.</p> In\u00a0[36]: Copied! <pre>fig = plt.figure()\n\nfor j, catalog in enumerate(all_stars):\n    try:\n        this = pd.read_csv(catalog)\n        \n        thispar = this.Parallax\n        thism0, thism1, thism2 = (np.log10(this.BlueF), \n                                  np.log10(this.GreenF), \n                                  np.log10(this.RedF))\n        thiscolour = thism2-thism0\n        dist = 1/thispar\n        abs_mag = thism1 + 2*np.log10(dist) \n        mm = thispar&gt;0.010 # only pick the ones with good signal-to-noise - 10 mas is ok \n        \n        h = plt.scatter(thiscolour[mm],abs_mag[mm],color='C1')\n    except:\n        pass\n\ns = plt.scatter(colour,m1+9.2,color='C0')\n\nplt.ylabel('Log Flux 1')\nplt.xlabel('Log Flux 2 - Log Flux 0')\n\nplt.legend([h,s],['Benchmark','Cluster'])\n</pre> fig = plt.figure()  for j, catalog in enumerate(all_stars):     try:         this = pd.read_csv(catalog)                  thispar = this.Parallax         thism0, thism1, thism2 = (np.log10(this.BlueF),                                    np.log10(this.GreenF),                                    np.log10(this.RedF))         thiscolour = thism2-thism0         dist = 1/thispar         abs_mag = thism1 + 2*np.log10(dist)          mm = thispar&gt;0.010 # only pick the ones with good signal-to-noise - 10 mas is ok                   h = plt.scatter(thiscolour[mm],abs_mag[mm],color='C1')     except:         pass  s = plt.scatter(colour,m1+9.2,color='C0')  plt.ylabel('Log Flux 1') plt.xlabel('Log Flux 2 - Log Flux 0')  plt.legend([h,s],['Benchmark','Cluster']) Out[36]: <pre>&lt;matplotlib.legend.Legend at 0x7ff3527d4a90&gt;</pre> In\u00a0[38]: Copied! <pre>10**(np.array([8.8,9.0])/2)\n</pre> 10**(np.array([8.8,9.0])/2) Out[38]: <pre>array([25118.8643151 , 31622.77660168])</pre> <p>By trial and error it looks like this offset is about (8.8-9.0).</p> <p>This is <code>2*log10(distance)</code> so this cluster is at a distance of 10^(8.8/2 - 9.0/2) = 25000-32000 pc.</p> <p>(I looked it up in the answers and it's 29000 pc - so a pretty decent estimate!)</p> <p>How do you think you could automate this process? Do you think you could</p> <ul> <li>Could you get a rotation curve of this galaxy? What is its mass? Does it have a black hole?<ul> <li>This is fun! I would love to see some rotation curves. Do you see anything interesting happening at the centre of the galaxies?</li> </ul> </li> <li>identify all the galaxies at once, perhaps by using a clustering algorithm?<ul> <li>This is great to do, but I strongly recommend you do not go straight for clustering until after you have achieved a good estimate of H0 by more manual means. This is a great way to go down a rabbit hole and lose time otherwise.</li> </ul> </li> <li>Get a more accurate HR diagram fitting, perhaps by using kernel density estimation?<ul> <li>Ditto.</li> </ul> </li> </ul>"},{"location":"tutorials/hr_diagram/#hr-diagram","title":"HR Diagram\u00b6","text":"<p>In this notebook we'll show you how to make an HR diagram of nearby field stars and a cluster, and use this to determine their relative distances.</p> <p>You don't have to cut and paste code - if you clone this git repository <code>https://github.com/benjaminpope/ladder</code>, these examples will be available under <code>docs/notebooks</code>.</p>"},{"location":"tutorials/hr_diagram/#load-some-data","title":"Load some data\u00b6","text":"<p>Let's pick a data directory from files I prepared earlier. These were from an earlier run of the universe-making code and don't belong to any group in this project!</p> <p>Here, <code>ddir</code> should be the data directory for the camera where your stars are. The <code>..</code> means the directory above the one you are currently working in - it is important that this path points to where your data live, or else the code won't know where to look!</p>"},{"location":"tutorials/hr_diagram/#nearby-stars-as-benchmarks","title":"Nearby stars as benchmarks\u00b6","text":"<p>We'll have to find out the distance to this cluster by fitting an HR diagram. Let's look at all the nearby stars and see if we can get something nice.</p> <p>We are going to make a list of every single <code>points.txt</code> file of stars in every camera.</p> <p>We use <code>glob</code> for this, which is a Python package for searching for filenames, and in this <code>*</code> is a 'wildcard' that means match anything here that otherwise fits the format - so this means search for <code>Down</code>, <code>Up</code>, and so forth, and <code>A01</code>, <code>B05</code> etc for the second <code>*</code>.</p>"},{"location":"tutorials/hr_diagram/#distance-estimation","title":"Distance Estimation\u00b6","text":"<p>Let's estimate the distance to this cluster. Remember the inverse square law - the brightness you see is proportional to</p> <pre><code>(brightness at distance d1)*(d1**2 / d2 **2)</code></pre> <p>What we want to do is standardize everything to how bright it would be at a distance of, for the sake of argument, 1 parsec. (In real research, for no particular reason, people usually use 10 parsecs for absolute magnitudes, but for simplicity we will use 1).</p> <p>If we're working in log units, <code>log(1/d**2) = -2*log(d)</code>, so this means that our standardized log flux is</p> <pre><code>abs_mag = observed_mag + 2*log(dist)</code></pre> <p>Now let's plot them on the same axes.</p>"},{"location":"tutorials/hr_diagram/#more-advanced-steps","title":"More advanced steps...\u00b6","text":""},{"location":"tutorials/introduction/","title":"Introduction","text":""},{"location":"tutorials/introduction/#to-start-off","title":"To start off...","text":"<p>This simulation was programmed in Python and it was designed with analysis in Python in mind. Of course, you can use any language you'd like to look at and analyse the data but we strongly recommend Python due to its versatility and use in astronomy. </p> <p>We have included example Python notebooks in a series of tutorials in how to analyse the data. As you are encouraged to learn in each lesson, more tutorial pages will progressively become available over the semester covering each new topic. A week after each lesson, we will update tutorials for the previous week to reflect any issues that have arisen and give fuller answers to problems you are solving, so that nobody is left behind by the end of semester. </p>"},{"location":"tutorials/introduction/#what-you-can-find","title":"What you can find","text":"<p>In principle, and with enough effort, you could find most of if not all of the physics that is simulated in this program. A full grade can be obtained without finding everything though! Below is a non-exhaustive list of what you could determine:</p> Stars: <ul> <li>Star temperatures</li> <li>Star Radii</li> <li>Star bolometric luminosities</li> <li>Star masses (big maybe on this one)</li> <li>Stellar variability periods (for those stars which have variable light curves)<ul> <li>Period-Luminosity function for variable stars</li> </ul> </li> <li>Stellar populations on a HR diagram</li> </ul> Galaxies: <ul> <li>Galaxy classification<ul> <li>Do different classes have different colour/luminosity?</li> <li>Are there different populations of stars in different areas of a galaxy?</li> </ul> </li> <li>Galaxy rotation curves<ul> <li>Do galaxies have black holes?<ul> <li>Do black holes have radio emission? Does radio emission depend on galaxy type?</li> <li>How massive are the black holes?</li> </ul> </li> <li>Do galaxies have dark matter?</li> </ul> </li> </ul> Galaxy clusters: <ul> <li>How many galaxies are in clusters?</li> <li>Galaxy cluster rotation curves<ul> <li>Do galaxy clusters have dark matter?</li> </ul> </li> <li>Galaxy type vs position in the cluster?</li> </ul> Universe overall: <ul> <li>What is the hubble constant? (this is the main goal of analysis!)</li> <li>Where are flashes? How often do they occur? What is their intrinsic brightness? Where do they come from?</li> <li>Is the universe homogeneous and/or isotropic? </li> <li>Are galaxies the same colour at any distance?</li> </ul>"},{"location":"tutorials/lombscargle_example/","title":"Extracting Period Information","text":"<p>First, let's load all the Python packages we'll need.</p> In\u00a0[1]: Copied! <pre>import numpy as np # for maths \nimport matplotlib # for plotting \nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm # tqdm is a package that lets you make progress bars to see how a loop is going\n\nimport os \n\nimport pandas as pd # pandas is a popular library in industry for manipulating large data tables\n\nfrom astropy.timeseries import LombScargle\n\n# configure notebook for plotting\n%matplotlib inline\n\nmpl.style.use('seaborn-colorblind') # colourblind-friendly colour scheme\n\n# subsequent lines default plot settings\nmatplotlib.rcParams['image.origin'] = 'lower'\nmatplotlib.rcParams['figure.figsize']=(8.0,6.0)   \nmatplotlib.rcParams['font.size']=16              \nmatplotlib.rcParams['savefig.dpi']= 300             \n\nimport warnings\nwarnings.filterwarnings('ignore')\n</pre> import numpy as np # for maths  import matplotlib # for plotting  import matplotlib as mpl import matplotlib.pyplot as plt  from tqdm import tqdm # tqdm is a package that lets you make progress bars to see how a loop is going  import os   import pandas as pd # pandas is a popular library in industry for manipulating large data tables  from astropy.timeseries import LombScargle  # configure notebook for plotting %matplotlib inline  mpl.style.use('seaborn-colorblind') # colourblind-friendly colour scheme  # subsequent lines default plot settings matplotlib.rcParams['image.origin'] = 'lower' matplotlib.rcParams['figure.figsize']=(8.0,6.0)    matplotlib.rcParams['font.size']=16               matplotlib.rcParams['savefig.dpi']= 300               import warnings warnings.filterwarnings('ignore') <pre>/var/folders/q3/wlh9qxyn1nxdm7v7l30qryf00000gn/T/ipykernel_1623/1040310382.py:17: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-&lt;style&gt;'. Alternatively, directly use the seaborn API instead.\n  mpl.style.use('seaborn-colorblind') # colourblind-friendly colour scheme\n</pre> In\u00a0[5]: Copied! <pre>ddir_stars = '/Users/benjaminpope/Downloads/universe_6683/' # point this to where you unzip your data!\n\nddir = ddir_stars + '/Variable_Star_Data/'\n</pre> ddir_stars = '/Users/benjaminpope/Downloads/universe_6683/' # point this to where you unzip your data!  ddir = ddir_stars + '/Variable_Star_Data/' <p>What's in the directory?</p> <p>Let's pick and load a single .csv file of a light curve. In this tutorial we'll use Astropy, a standard Python library for astronomical data; if you know Pandas, this is more common in data science and has similar functionality. Use whatever you like!</p> <p>We're first going to calculate the Nyquist limit - you can't pick out a period shorter than twice the rate at which you get time samples. As these data are hourly, it means we can't measure periods shorter than 2 hours. Because it's linear in Fourier space, we'll usually work in frequency rather than period, but use whichever you wish - so long as you're careful!</p> In\u00a0[9]: Copied! <pre>fname = 'FrontS036632.csv' # put your filename here\n\ndata = pd.read_csv(ddir+fname) # load in CSV data as a Pandas object\nprint(data.keys()) # see what's in it\ntime, flux = data.Time, data.NormalisedFlux # just extract the columns as variables\ndt = np.median(np.diff(time))\nprint('Nyquist Limit',0.5/dt,'cycles per hour') # can't get frequencies higher than the Nyquist limit\n</pre> fname = 'FrontS036632.csv' # put your filename here  data = pd.read_csv(ddir+fname) # load in CSV data as a Pandas object print(data.keys()) # see what's in it time, flux = data.Time, data.NormalisedFlux # just extract the columns as variables dt = np.median(np.diff(time)) print('Nyquist Limit',0.5/dt,'cycles per hour') # can't get frequencies higher than the Nyquist limit <pre>Index(['Time', 'NormalisedFlux'], dtype='object')\nNyquist Limit 0.5 cycles per hour\n</pre> In\u00a0[10]: Copied! <pre>plt.plot(time,flux,'.',markersize=16)\nplt.xlabel('Time (h)')\nplt.ylabel('Relative Flux')\n</pre> plt.plot(time,flux,'.',markersize=16) plt.xlabel('Time (h)') plt.ylabel('Relative Flux') Out[10]: <pre>Text(0, 0.5, 'Relative Flux')</pre> In\u00a0[11]: Copied! <pre>LS = LombScargle(time,flux) # initialize a Lomb-Scargle algorithm from Astropy\nfreqs = np.linspace(1/100,0.45,10000) # frequency grid shouldn't go higher than Nyquist limit\npower = LS.power(freqs) # calculate LS power\n</pre> LS = LombScargle(time,flux) # initialize a Lomb-Scargle algorithm from Astropy freqs = np.linspace(1/100,0.45,10000) # frequency grid shouldn't go higher than Nyquist limit power = LS.power(freqs) # calculate LS power   In\u00a0[12]: Copied! <pre>print('Best period: %.2f h' % (1/freqs[np.argmax(power)]))\n</pre> print('Best period: %.2f h' % (1/freqs[np.argmax(power)])) <pre>Best period: 44.54 h\n</pre> In\u00a0[13]: Copied! <pre>plt.plot(freqs,power)\nplt.xlabel('Frequency (c/h)')\nplt.ylabel('LS Power')\n</pre> plt.plot(freqs,power) plt.xlabel('Frequency (c/h)') plt.ylabel('LS Power') Out[13]: <pre>Text(0, 0.5, 'LS Power')</pre> In\u00a0[14]: Copied! <pre>import glob # this package lets you search for filenames\n\nfnames = glob.glob(ddir+'*.csv')\nprint(fnames[:10])\n</pre> import glob # this package lets you search for filenames  fnames = glob.glob(ddir+'*.csv') print(fnames[:10]) <pre>['/Users/benjaminpope/Downloads/universe_6683//Variable_Star_Data/FrontS032152.csv', '/Users/benjaminpope/Downloads/universe_6683//Variable_Star_Data/FrontS113188.csv', '/Users/benjaminpope/Downloads/universe_6683//Variable_Star_Data/FrontS026899.csv', '/Users/benjaminpope/Downloads/universe_6683//Variable_Star_Data/BottomS011715.csv', '/Users/benjaminpope/Downloads/universe_6683//Variable_Star_Data/LeftS064307.csv', '/Users/benjaminpope/Downloads/universe_6683//Variable_Star_Data/FrontS112282.csv', '/Users/benjaminpope/Downloads/universe_6683//Variable_Star_Data/RightS092351.csv', '/Users/benjaminpope/Downloads/universe_6683//Variable_Star_Data/BottomS009628.csv', '/Users/benjaminpope/Downloads/universe_6683//Variable_Star_Data/LeftS045487.csv', '/Users/benjaminpope/Downloads/universe_6683//Variable_Star_Data/RightS117415.csv']\n</pre> <p>Below we are going to make plots of a subset of data these data - this is always a good idea before running a large computation, just to see what you have visually. This is the first thing every data scientist does with new data!</p> <p>I encourage you to look at these plots, as you'll see that one dataset is too noisy, and we don't get a good sinusoidal fit to it. You can comment out the plotting code if you run it on lots of datasets though!</p> In\u00a0[15]: Copied! <pre>freqs = np.linspace(1/100,0.45,10000) # frequency grid shouldn't go higher than Nyquist limit\nperiods = [] # start an empty list to hold the period \n\nfig, axes = plt.subplots(3,4,figsize=(18,12))\nfor fname, ax in zip(fnames[:12], axes.ravel()): # you can loop over two things\n    data = pd.read_csv(fname) # load in CSV data as a Pandas object\n\n    time, flux = data.Time, data.NormalisedFlux # just extract the columns as variables\n\n    LS = LombScargle(time,flux) # initialize a Lomb-Scargle\n    power = LS.power(freqs) # calculate LS power \n    bestfreq = freqs[np.argmax(power)] # which frequency has the highest Lomb-Scargle power?\n    \n    pred = LS.model(time,bestfreq) # make a sine wave prediction at the best frequency\n    ax.plot(time,flux,'.')\n    ax.plot(time,pred) # plot the model over the data\n    \n    periods.append(1/bestfreq) # add each period to the list\n    \nperiods = np.array(periods) # turn it from a list to an array\n</pre> freqs = np.linspace(1/100,0.45,10000) # frequency grid shouldn't go higher than Nyquist limit periods = [] # start an empty list to hold the period   fig, axes = plt.subplots(3,4,figsize=(18,12)) for fname, ax in zip(fnames[:12], axes.ravel()): # you can loop over two things     data = pd.read_csv(fname) # load in CSV data as a Pandas object      time, flux = data.Time, data.NormalisedFlux # just extract the columns as variables      LS = LombScargle(time,flux) # initialize a Lomb-Scargle     power = LS.power(freqs) # calculate LS power      bestfreq = freqs[np.argmax(power)] # which frequency has the highest Lomb-Scargle power?          pred = LS.model(time,bestfreq) # make a sine wave prediction at the best frequency     ax.plot(time,flux,'.')     ax.plot(time,pred) # plot the model over the data          periods.append(1/bestfreq) # add each period to the list      periods = np.array(periods) # turn it from a list to an array <p>It looks like a few of them are dominated by noise!</p> <p>Now let's loop over all of them without plotting:</p> In\u00a0[16]: Copied! <pre>freqs = np.linspace(1/100,0.45,10000) # frequency grid shouldn't go higher than Nyquist limit\nperiods = [] # start an empty list to hold the period \nnames = []\n\nfor fname in tqdm(fnames): # tqdm is a package that gives you a progress bar - neat! \n    data = pd.read_csv(fname) # load in CSV data as a Pandas object\n\n    time, flux = data.Time, data.NormalisedFlux # just extract the columns as variables\n\n    LS = LombScargle(time,flux) # initialize a Lomb-Scargle\n    power = LS.power(freqs) # calculate LS power \n    bestfreq = freqs[np.argmax(power)] # which frequency has the highest Lomb-Scargle power?\n    \n    pred = LS.model(time,bestfreq) # make a sine wave prediction at the best frequency\n    \n    periods.append(1/bestfreq) # add each period to the list\n    names.append(os.path.basename(fname).strip('.csv')) # os.path.basename gets rid of directories and gives you the filename; then we strip '.csv'\n    \nperiods = np.array(periods) # turn it from a list to an array\n</pre> freqs = np.linspace(1/100,0.45,10000) # frequency grid shouldn't go higher than Nyquist limit periods = [] # start an empty list to hold the period  names = []  for fname in tqdm(fnames): # tqdm is a package that gives you a progress bar - neat!      data = pd.read_csv(fname) # load in CSV data as a Pandas object      time, flux = data.Time, data.NormalisedFlux # just extract the columns as variables      LS = LombScargle(time,flux) # initialize a Lomb-Scargle     power = LS.power(freqs) # calculate LS power      bestfreq = freqs[np.argmax(power)] # which frequency has the highest Lomb-Scargle power?          pred = LS.model(time,bestfreq) # make a sine wave prediction at the best frequency          periods.append(1/bestfreq) # add each period to the list     names.append(os.path.basename(fname).strip('.csv')) # os.path.basename gets rid of directories and gives you the filename; then we strip '.csv'      periods = np.array(periods) # turn it from a list to an array <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4392/4392 [00:58&lt;00:00, 75.19it/s] \n</pre> In\u00a0[17]: Copied! <pre>import glob # this package lets you search for filenames\nimport os\n\nvariables = pd.DataFrame({'Name':names,\n              'Period':periods}) # you can turn a dictionary into a dataframe like this\nvariables.Name = variables.Name.astype('|S') # have to do this so that it knows the names are strings\n\nall_star_files = glob.glob(ddir_stars+'*/Star_Data.csv')\n\nall_stars = pd.concat([pd.read_csv(table) for table in all_star_files]) # we are concatenating a list of dataframes; \n#we generate this list with a \"list comprehension\", a loop you write inside a list bracket \n\nall_stars.Name = all_stars.Name.astype('|S') # have to do this so that it knows the names are strings\nall_stars = all_stars[all_stars.Parallax &gt; 0.01] # 10 mas parallax cut\nprint(len(all_stars),'stars above 10 mas parallax') # check how many stars there are total with good parallax\n\nvariables = pd.merge(all_stars,variables,on='Name') # merge these two arrays according to the keyword 'name'\nprint('Of which',len(variables),'variables') # cut down to a small list\n</pre> import glob # this package lets you search for filenames import os  variables = pd.DataFrame({'Name':names,               'Period':periods}) # you can turn a dictionary into a dataframe like this variables.Name = variables.Name.astype('|S') # have to do this so that it knows the names are strings  all_star_files = glob.glob(ddir_stars+'*/Star_Data.csv')  all_stars = pd.concat([pd.read_csv(table) for table in all_star_files]) # we are concatenating a list of dataframes;  #we generate this list with a \"list comprehension\", a loop you write inside a list bracket   all_stars.Name = all_stars.Name.astype('|S') # have to do this so that it knows the names are strings all_stars = all_stars[all_stars.Parallax &gt; 0.01] # 10 mas parallax cut print(len(all_stars),'stars above 10 mas parallax') # check how many stars there are total with good parallax  variables = pd.merge(all_stars,variables,on='Name') # merge these two arrays according to the keyword 'name' print('Of which',len(variables),'variables') # cut down to a small list  <pre>908 stars above 10 mas parallax\nOf which 43 variables\n</pre> <p>Let's make an HR diagram. One thing to keep in mind is that our convention of using magnitudes is just to match our modern scale to the Ancient Greeks - in your fantasy universe, you can use whatever units you like (but I recommend that they be logarithmic!). This is an important note - lots of students get hung up on the -2.5 and the solar luminosity, but these are just conventions in our world that aren't important for the physics.</p> <p>So let's have a simple log scale for magnitude here.</p> In\u00a0[18]: Copied! <pre>m0, m1, m2 = np.log10(all_stars['BlueF']), np.log10(all_stars['GreenF']), np.log10(all_stars['RedF']) \ncolour = m2-m0\nabs_mag = m1 + 2*np.log10(1./all_stars.Parallax) \n\nv0, v1, v2 = np.log10(variables['BlueF']), np.log10(variables['GreenF']), np.log10(variables['RedF']) \nvariable_colour = v2-v0\nabs_mag_v = v1 + 2*np.log10(1./variables.Parallax)\n</pre> m0, m1, m2 = np.log10(all_stars['BlueF']), np.log10(all_stars['GreenF']), np.log10(all_stars['RedF'])  colour = m2-m0 abs_mag = m1 + 2*np.log10(1./all_stars.Parallax)   v0, v1, v2 = np.log10(variables['BlueF']), np.log10(variables['GreenF']), np.log10(variables['RedF'])  variable_colour = v2-v0 abs_mag_v = v1 + 2*np.log10(1./variables.Parallax)   In\u00a0[19]: Copied! <pre>s = plt.plot(colour,abs_mag,'.C0')\nh = plt.plot(variable_colour,abs_mag_v,'.C2',marker='*',markersize=10)\n\n    \nplt.legend([s, h],['Steady','Variable'])\nplt.ylabel('Log Flux 1')\nplt.xlabel('Log Flux 2 - Log Flux 0')\n</pre> s = plt.plot(colour,abs_mag,'.C0') h = plt.plot(variable_colour,abs_mag_v,'.C2',marker='*',markersize=10)       plt.legend([s, h],['Steady','Variable']) plt.ylabel('Log Flux 1') plt.xlabel('Log Flux 2 - Log Flux 0') Out[19]: <pre>Text(0.5, 0, 'Log Flux 2 - Log Flux 0')</pre> <p>So we see there are two kinds of variable in this stellar population - I wonder what their properties are?</p> In\u00a0[20]: Copied! <pre>plt.plot(variables.Period,abs_mag_v,'.',color='C2')\nplt.xlabel('Period (h)')\nplt.ylabel('Log Flux');\n</pre> plt.plot(variables.Period,abs_mag_v,'.',color='C2') plt.xlabel('Period (h)') plt.ylabel('Log Flux'); <p>Yes we can! We see two kinds of star - with a short and a long period - and possibly a linear trend in each dataset. Will this pan out? You'll have to look at more data! And it could be very different in your own analysis - all these universes are generated with random, made-up (but consistent) stellar physics.</p> <p>To think about:</p> <ul> <li>How would you quantify uncertainties on period?</li> <li>How would you fit period-luminosity relations to these data?</li> </ul>"},{"location":"tutorials/lombscargle_example/#extracting-period-information","title":"Extracting Period Information\u00b6","text":"<p>In this tutorial, we will go through</p> <ul> <li>loading variable star data using Astropy</li> <li>extracting period information using the Lomb-Scargle Periodogram</li> <li>looking at where variables lie in the HR diagram</li> <li>making a period-luminosity relation</li> </ul> <p>You don't have to cut and paste code - if you clone this git repository <code>https://github.com/benjaminpope/ladder</code>, these examples will be available under <code>docs/notebooks</code>.</p>"},{"location":"tutorials/lombscargle_example/#load-some-data","title":"Load some data\u00b6","text":"<p>Let's pick a data directory from files I prepared earlier. These were from an earlier run of the universe-making code and don't belong to any group in this project!</p>"},{"location":"tutorials/lombscargle_example/#lomb-scargle-periodograms","title":"Lomb-Scargle Periodograms\u00b6","text":"<p>The Lomb-Scargle periodogram is named because it was first used in the PhD thesis of the former director of Sydney Observatory, Nick Lomb. It is probably the most widely cited Australian astronomy paper! If you want to determine the periodic content (Fourier power spectrum) of a time series - including if it is sparsely or irregularly sampled - it is the algorithm to go to. Rather than using the Fast Fourier Transform, the LS periodogram simply does a least squares fit against sines and cosines - this can be done very fast, and applied to nonuniform data.</p>"},{"location":"tutorials/lombscargle_example/#now-lets-loop-over-all-the-stars-in-the-directory","title":"Now let's loop over all the stars in the directory!\u00b6","text":""},{"location":"tutorials/lombscargle_example/#hr-diagram-and-period-luminosity-relation","title":"HR Diagram and Period-Luminosity Relation\u00b6","text":""},{"location":"tutorials/lombscargle_example/#period-luminosity-diagram","title":"Period-Luminosity Diagram\u00b6","text":"<p>Now let's make a period-luminosity diagram. Can we distinguish these stars by period, and anchor the distance ladder?</p>"},{"location":"tutorials/uncubemapping/","title":"Un-Cubemapping Data","text":"In\u00a0[1]: Copied! <pre>datapath = 'Sim Data (Clusters; 800, Seed; 2639)' # all of the data is within a folder in this .ipynb file's directory\n</pre> datapath = 'Sim Data (Clusters; 800, Seed; 2639)' # all of the data is within a folder in this .ipynb file's directory In\u00a0[2]: Copied! <pre>import matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nfig, axes = plt.subplots(3, 4, figsize=(12, 9)) # generate a figure to fit 3 high by 4 wide square images\nfig.subplots_adjust(wspace=0, hspace=0) # we want these squares to be adjacent to each other, with no gap\n# now to iterate over each subplot and remove the axis bars and ticks/labels\nfor row in axes:\n    for ax in row:\n        for side in ['top','right','bottom','left']:\n            ax.spines[side].set_visible(False)\n        ax.tick_params(axis='both', which='both', labelbottom=False, bottom=False, left=False, labelleft=False)\n\n# now we load in the images and put them at their correct location\nfor i, direct in enumerate([\"Back\", \"Left\", \"Front\", \"Right\", \"Top\", \"Bottom\"]): # one loop for each direction\n    img = mpimg.imread(datapath + f'/{direct}/{direct}.png') # this loads in the image from the corresponding folder\n    img_cropped = img[:-700, 900:] # crop the image to remove the axis labels/ticks\n    if i == 4: # if the Top image\n        imgplot = axes[0][1].imshow(img_cropped) # image needs to go at the top\n    elif i == 5: # if the Bottom image\n        imgplot = axes[2][1].imshow(img_cropped) # image needs to go at the bottom\n    else:\n        imgplot = axes[1][i].imshow(img_cropped) # put the image in the middle row at the correct column\n</pre> import matplotlib.pyplot as plt import matplotlib.image as mpimg  fig, axes = plt.subplots(3, 4, figsize=(12, 9)) # generate a figure to fit 3 high by 4 wide square images fig.subplots_adjust(wspace=0, hspace=0) # we want these squares to be adjacent to each other, with no gap # now to iterate over each subplot and remove the axis bars and ticks/labels for row in axes:     for ax in row:         for side in ['top','right','bottom','left']:             ax.spines[side].set_visible(False)         ax.tick_params(axis='both', which='both', labelbottom=False, bottom=False, left=False, labelleft=False)  # now we load in the images and put them at their correct location for i, direct in enumerate([\"Back\", \"Left\", \"Front\", \"Right\", \"Top\", \"Bottom\"]): # one loop for each direction     img = mpimg.imread(datapath + f'/{direct}/{direct}.png') # this loads in the image from the corresponding folder     img_cropped = img[:-700, 900:] # crop the image to remove the axis labels/ticks     if i == 4: # if the Top image         imgplot = axes[0][1].imshow(img_cropped) # image needs to go at the top     elif i == 5: # if the Bottom image         imgplot = axes[2][1].imshow(img_cropped) # image needs to go at the bottom     else:         imgplot = axes[1][i].imshow(img_cropped) # put the image in the middle row at the correct column <p>Now we can start doing the coordinate transform.</p> In\u00a0[3]: Copied! <pre>import numpy as np\ndef cube_to_equirect(direction, u, v):\n    # convert range -45 to 45 to -1 to 1\n    uc = u / 45\n    vc = v / 45\n    if direction == \"Front\": # POSITIVE X\n        x = 1\n        y = vc\n        z = -uc \n    elif direction == \"Back\":  # NEGATIVE X\n        x = -1\n        y = vc\n        z = uc\n    elif direction == \"Top\": # POSITIVE Y\n        x = uc\n        y = 1\n        z = -vc\n    elif direction == \"Bottom\": # NEGATIVE Y\n        x = uc\n        y = -1\n        z = vc\n    elif direction == \"Left\": # POSITIVE Z\n        x = uc\n        y = vc\n        z = 1\n    else: # direction == \"Right\": # NEGATIVE Z\n        x = -uc\n        y = vc\n        z = -1 \n    # now to convert the XYZ to spherical coordinates\n    # this is using the physics convention of spherical coords!\n    r = np.sqrt(x**2 + y**2 + z**2)\n    azimuth = np.arctan2(z, x)\n    theta = np.arccos(y / r)\n\n    theta = theta * 180 / np.pi\n    azimuth = (- azimuth + np.pi) * 360 / (2 * np.pi)\n    \n    return azimuth, theta\n</pre> import numpy as np def cube_to_equirect(direction, u, v):     # convert range -45 to 45 to -1 to 1     uc = u / 45     vc = v / 45     if direction == \"Front\": # POSITIVE X         x = 1         y = vc         z = -uc      elif direction == \"Back\":  # NEGATIVE X         x = -1         y = vc         z = uc     elif direction == \"Top\": # POSITIVE Y         x = uc         y = 1         z = -vc     elif direction == \"Bottom\": # NEGATIVE Y         x = uc         y = -1         z = vc     elif direction == \"Left\": # POSITIVE Z         x = uc         y = vc         z = 1     else: # direction == \"Right\": # NEGATIVE Z         x = -uc         y = vc         z = -1      # now to convert the XYZ to spherical coordinates     # this is using the physics convention of spherical coords!     r = np.sqrt(x**2 + y**2 + z**2)     azimuth = np.arctan2(z, x)     theta = np.arccos(y / r)      theta = theta * 180 / np.pi     azimuth = (- azimuth + np.pi) * 360 / (2 * np.pi)          return azimuth, theta <p>Now we can finally start to convert some data! To start with, let's look at the star data and plot an image of all of the resolved stars in our new unified system.</p> In\u00a0[4]: Copied! <pre>import pandas as pd\n\nfor i, direct in enumerate([\"Front\", \"Back\", \"Left\", \"Right\", \"Top\", \"Bottom\"]):\n        # read the data from the .txt file into a dataframe\n        stardata = pd.read_csv(datapath + f'/{direct}/Star Data.txt', delimiter=' ')  \n        u = stardata[\"X\"].to_numpy(); v = stardata[\"Y\"].to_numpy() # convert X and Y data to \"U\" and \"V\" data\n        azimuth, theta = cube_to_equirect(direct, u, v) # perform the coordinate transform\n        azimuth = np.around(azimuth, decimals=4); theta = np.around(theta, decimals=4) # round to appropriate decimals\n        \n        df = pd.DataFrame({\"Equat\": azimuth, \"Polar\": theta}) # make a temporary DataFrame object with new coordinates\n        # now overwrite the old coordinates with the new ones\n        stardata['X'] = df['Equat']\n        stardata[\"Y\"] = df[\"Polar\"]\n        stardata = stardata.rename(columns={\"X\": \"Equat\", \"Y\": \"Polar\"}) # and finally change the name of the columns \n        if i == 0:\n            # if this is the first iteration, write to a new DataFrame that will store all of the star data\n            all_stardata = stardata\n        else:\n            all_stardata = pd.concat([all_stardata, stardata]) # add this face stardata to the rest of the data\n\n# now let's plot the data to see if it's worked!\nfig, ax = plt.subplots(figsize=(12, 6))\nax.scatter(all_stardata[\"Equat\"].to_numpy(), all_stardata[\"Polar\"].to_numpy(), s=0.1, c='k', lw=0);\nax.set_xlim(0, 360); ax.set_ylim(0, 180);\nax.invert_yaxis();\n</pre> import pandas as pd  for i, direct in enumerate([\"Front\", \"Back\", \"Left\", \"Right\", \"Top\", \"Bottom\"]):         # read the data from the .txt file into a dataframe         stardata = pd.read_csv(datapath + f'/{direct}/Star Data.txt', delimiter=' ')           u = stardata[\"X\"].to_numpy(); v = stardata[\"Y\"].to_numpy() # convert X and Y data to \"U\" and \"V\" data         azimuth, theta = cube_to_equirect(direct, u, v) # perform the coordinate transform         azimuth = np.around(azimuth, decimals=4); theta = np.around(theta, decimals=4) # round to appropriate decimals                  df = pd.DataFrame({\"Equat\": azimuth, \"Polar\": theta}) # make a temporary DataFrame object with new coordinates         # now overwrite the old coordinates with the new ones         stardata['X'] = df['Equat']         stardata[\"Y\"] = df[\"Polar\"]         stardata = stardata.rename(columns={\"X\": \"Equat\", \"Y\": \"Polar\"}) # and finally change the name of the columns          if i == 0:             # if this is the first iteration, write to a new DataFrame that will store all of the star data             all_stardata = stardata         else:             all_stardata = pd.concat([all_stardata, stardata]) # add this face stardata to the rest of the data  # now let's plot the data to see if it's worked! fig, ax = plt.subplots(figsize=(12, 6)) ax.scatter(all_stardata[\"Equat\"].to_numpy(), all_stardata[\"Polar\"].to_numpy(), s=0.1, c='k', lw=0); ax.set_xlim(0, 360); ax.set_ylim(0, 180); ax.invert_yaxis(); <p>I have it on good authority that this has worked correctly! Now to save the data to a .txt file on the system. Using <code>pandas</code>, saving a DataFrame is super easy and can be done in one line:</p> In\u00a0[5]: Copied! <pre>all_stardata.to_csv(datapath + \"/Converted Star Data.txt\", index=False, sep=' ')\n# dont want to save the 'indices' of the data, and I want a space character to separate the data\n</pre> all_stardata.to_csv(datapath + \"/Converted Star Data.txt\", index=False, sep=' ') # dont want to save the 'indices' of the data, and I want a space character to separate the data <p>Now we can repeat this process with the distant galaxy data!</p> In\u00a0[6]: Copied! <pre>for i, direct in enumerate([\"Front\", \"Back\", \"Left\", \"Right\", \"Top\", \"Bottom\"]):\n        # read the data from the .txt file into a dataframe\n        galaxdata = pd.read_csv(datapath + f'/{direct}/Distant Galaxy Data.txt', delimiter=' ')  \n        u = galaxdata[\"X\"].to_numpy(); v = galaxdata[\"Y\"].to_numpy() # convert X and Y data to \"U\" and \"V\" data\n        azimuth, theta = cube_to_equirect(direct, u, v) # perform the coordinate transform\n        azimuth = np.around(azimuth, decimals=4); theta = np.around(theta, decimals=4) # round to appropriate decimals\n        \n        df = pd.DataFrame({\"Equat\": azimuth, \"Polar\": theta}) # make a temporary DataFrame object with new coordinates\n        # now overwrite the old coordinates with the new ones\n        galaxdata['X'] = df['Equat']\n        galaxdata[\"Y\"] = df[\"Polar\"]\n        galaxdata = galaxdata.rename(columns={\"X\": \"Equat\", \"Y\": \"Polar\"}) # and finally change the name of the columns \n        if i == 0:\n            # if this is the first iteration, write to a new DataFrame that will store all of the star data\n            all_galaxdata = galaxdata\n        else:\n            all_galaxdata = pd.concat([all_galaxdata, galaxdata]) # add this face stardata to the rest of the data\n\n# now let's plot the data to see if it's worked!\nfig, ax = plt.subplots(figsize=(12, 6))\nax.scatter(all_galaxdata[\"Equat\"].to_numpy(), all_galaxdata[\"Polar\"].to_numpy(), s=0.5, c='k', lw=0);\nax.set_xlim(0, 360); ax.set_ylim(0, 180);\nax.invert_yaxis();\n</pre> for i, direct in enumerate([\"Front\", \"Back\", \"Left\", \"Right\", \"Top\", \"Bottom\"]):         # read the data from the .txt file into a dataframe         galaxdata = pd.read_csv(datapath + f'/{direct}/Distant Galaxy Data.txt', delimiter=' ')           u = galaxdata[\"X\"].to_numpy(); v = galaxdata[\"Y\"].to_numpy() # convert X and Y data to \"U\" and \"V\" data         azimuth, theta = cube_to_equirect(direct, u, v) # perform the coordinate transform         azimuth = np.around(azimuth, decimals=4); theta = np.around(theta, decimals=4) # round to appropriate decimals                  df = pd.DataFrame({\"Equat\": azimuth, \"Polar\": theta}) # make a temporary DataFrame object with new coordinates         # now overwrite the old coordinates with the new ones         galaxdata['X'] = df['Equat']         galaxdata[\"Y\"] = df[\"Polar\"]         galaxdata = galaxdata.rename(columns={\"X\": \"Equat\", \"Y\": \"Polar\"}) # and finally change the name of the columns          if i == 0:             # if this is the first iteration, write to a new DataFrame that will store all of the star data             all_galaxdata = galaxdata         else:             all_galaxdata = pd.concat([all_galaxdata, galaxdata]) # add this face stardata to the rest of the data  # now let's plot the data to see if it's worked! fig, ax = plt.subplots(figsize=(12, 6)) ax.scatter(all_galaxdata[\"Equat\"].to_numpy(), all_galaxdata[\"Polar\"].to_numpy(), s=0.5, c='k', lw=0); ax.set_xlim(0, 360); ax.set_ylim(0, 180); ax.invert_yaxis(); <p>Looks pretty good! We can see something going on near the horizontal middle of the image (which will be looked at in a future notebook!).</p> <p>Once again, we now save this to a .txt file on disk.</p> In\u00a0[7]: Copied! <pre>all_galaxdata.to_csv(datapath + \"/Converted Distant Galaxy Data.txt\", index=False, sep=' ')\n# dont want to save the 'indices' of the data, and I want a space character to separate the data\n</pre> all_galaxdata.to_csv(datapath + \"/Converted Distant Galaxy Data.txt\", index=False, sep=' ') # dont want to save the 'indices' of the data, and I want a space character to separate the data <p>And with that we're done! With this task now done, we can more easily start looking at the actual physics within the simulation. This will be covered in the next few notebooks.</p>"},{"location":"tutorials/uncubemapping/#un-cubemapping-data","title":"Un-Cubemapping Data\u00b6","text":"<p>Having cubemapped data is useful if, for instance, you want to look at a specific area on the sky. This lends well to looking at a single galaxy, or a single galaxy cluster that fit entirely within one of the six faces of the cubemapped sky. However, to look at the universe as whole, it can be easier to have all of the data within a unified coordinate system. Maybe the easiest transformation is that from a cubemap coordinate system (e.g. Front face, X/Y coordinates) to an equirectangular coordinate system (one image, X/Y coordinates).</p> <p>With that in mind, let's get started by defining the location of our data.</p>"},{"location":"tutorials/uncubemapping/#cubemapped-data","title":"Cubemapped Data\u00b6","text":"<p>In the codeblock below, I've stitched together the 6 cube face images to show how they align with each other. In the middle row, our faces are (in order): Back, Left, Front, Right.</p> <p>We can see that the bottom of the Top image fits onto the top of the Left image, and the top of the Bottom image fits onto the bottom of the Left image.</p>"},{"location":"tutorials/uncubemapping/#coordinate-transform-to-equirectangular","title":"Coordinate Transform to Equirectangular\u00b6","text":"<p>The way we can transform from the cubemap coordinates to spherical coordinates is by doing some trigonometry, and by first identifying which portion of XYZ space our coordinates are in. The algorithm is derived from this wikipedia page on cubemapping, with the XYZ to spherical coordinate transfrom derived from this wikipedia page on Spherical coordinates. I needed to switch around the $y$ and the $z$ in the spherical coordinate transform, because physicists and mathematicians tend not to agree on a lot of things (looking at you calculus).</p>"}]}